{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CSE545-SDG3-Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=SDG3, master=local[*]) created by __init__ at <ipython-input-10-6f706fe7a6d5>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6f706fe7a6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SDG3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msqlCtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=SDG3, master=local[*]) created by __init__ at <ipython-input-10-6f706fe7a6d5>:1 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"SDG3\")\n",
    "sqlCtx = SQLContext(sc)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../LinkPE14US/VS14LINK.USNUMPUB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'../LinkPE14US'\n",
    "print(os.path.join(data_path, \"VS14LINK.USNUMPUB\"))\n",
    "df = sqlCtx.read.text(os.path.join(data_path, \"VS14LINK.USNUMPUB\"))  #350 mb\n",
    "#df = sqlCtx.read.text(\"/VS14LINK.USDENPUB\") # 5gb\n",
    "#df = sqlCtx.read.text(\"/*.*PUB\") # both\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_as_null(x):\n",
    "    return when(col(x) != ' ', col(x)).otherwise(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas data frame after adding column metadata\n",
    "pndf = df.select(\n",
    "    df.value.substr(9,4).cast(IntegerType()).alias('Birth_Year'),\n",
    "    df.value.substr(13,2).cast(IntegerType()).alias('Birth_Month'),\n",
    "    df.value.substr(75,2).cast(IntegerType()).alias('Mothers_Age'),\n",
    "    #df.value.substr(299,3).cast(IntegerType()).alias('Delivery_Weight_lbs'),\n",
    "    df.value.substr(332,2).cast(IntegerType()).alias('Num_Prev_Cesareans'),\n",
    "    #435\n",
    "    df.value.substr(435,1).cast(IntegerType()).alias('Payment_Source'),\n",
    "    df.value.substr(436,1).cast(IntegerType()).alias('Payment_Recode'),\n",
    "    df.value.substr(444,2).cast(IntegerType()).alias('Five_Minute_APGAR_Score'),\n",
    "    df.value.substr(446,1).cast(IntegerType()).alias('Five_Minute_APGAR_Recode'),\n",
    "    df.value.substr(448,2).cast(IntegerType()).alias('Ten_Minute_APGAR_Score'),\n",
    "    df.value.substr(450,1).cast(IntegerType()).alias('Ten_Minute_APGAR_Recode'),\n",
    "    df.value.substr(454,1).cast(IntegerType()).alias('Plurality'),\n",
    "    df.value.substr(456,1).cast(IntegerType()).alias('Plurality_Imputed'),\n",
    "    df.value.substr(475,1).cast(StringType()).alias('Sex_of_Infant'),\n",
    "    df.value.substr(476,1).cast(IntegerType()).alias('Imputed_Sex'),\n",
    "    df.value.substr(477,2).cast(IntegerType()).alias('Last_Normal_Menses_Month'),\n",
    "    df.value.substr(481,4).cast(IntegerType()).alias('Last_Normal_Menses_Year'),\n",
    "    df.value.substr(488,4).cast(IntegerType()).alias('Combined_Gestation_Imputed'),\n",
    "    df.value.substr(489,1).cast(IntegerType()).alias('Obstetric_Estimate_of_Gestation_Used_Flag'),\n",
    "    df.value.substr(490,2).cast(IntegerType()).alias('Combined_Gestation_Detail_in_Weeks'),\n",
    "    df.value.substr(492,1).cast(IntegerType()).alias('Combined_Gestation_Recode_Weeks'),\n",
    "    df.value.substr(509,2).cast(IntegerType()).alias('Birth_Weight_Recode_14'),\n",
    "    df.value.substr(511,1).cast(IntegerType()).alias('Birth_Weight_Recode_4'),\n",
    "    df.value.substr(512,4).cast(IntegerType()).alias('Imputed_Birthwieght'),\n",
    "    #Abnormal Conditions of the Newborn\n",
    "    df.value.substr(517,1).cast(StringType()).alias('Assisted_Ventilation'),\n",
    "    df.value.substr(518,1).cast(StringType()).alias('Assisted_Ventilation_Greater_Than_6_Hours'),\n",
    "    df.value.substr(519,1).cast(StringType()).alias('Admission_to_NICU'),\n",
    "    df.value.substr(520,1).cast(StringType()).alias('Surfactant'),\n",
    "    df.value.substr(521,1).cast(StringType()).alias('Antibiotics'),\n",
    "    df.value.substr(522,1).cast(StringType()).alias('Seizures'),\n",
    "    df.value.substr(531,1).cast(IntegerType()).alias('No_Abnormal_Conditions_Checked'),\n",
    "    #\n",
    "    #Congenital Anomalies of the Newborn\n",
    "    df.value.substr(537,1).cast(StringType()).alias('Anencephaly'),\n",
    "    df.value.substr(538,1).cast(StringType()).alias('Meningomyelocele_or_Spina_Bifida'),\n",
    "    df.value.substr(539,1).cast(StringType()).alias('Cyanotic_Congenital_Heart_Disease'),\n",
    "    df.value.substr(540,1).cast(StringType()).alias('Congenital_Diaphragmatic_Hernia'),\n",
    "    df.value.substr(541,1).cast(StringType()).alias('Omphalocele'),\n",
    "    df.value.substr(542,1).cast(StringType()).alias('Gastroschisis'),\n",
    "    df.value.substr(549,1).cast(StringType()).alias('Limb_Reduction_Defect'),\n",
    "    df.value.substr(550,1).cast(StringType()).alias('Cleft_Lip_w_or_wo_Cleft_Palate'),\n",
    "    df.value.substr(551,1).cast(StringType()).alias('Cleft_Palate_alone'),\n",
    "    df.value.substr(553,1).cast(StringType()).alias('Suspected_Chromosomal_Disorder'),\n",
    "    df.value.substr(554,1).cast(StringType()).alias('Hypospadias'),\n",
    "    df.value.substr(561,1).cast(IntegerType()).alias('No_Congenital_Anomalies_Checked'),\n",
    "    #\n",
    "    df.value.substr(568,1).cast(StringType()).alias('Infant_Living_at_Time_of_Report'),\n",
    "    df.value.substr(569,1).cast(StringType()).alias('Infant_Being_Breastfed')\n",
    "    #870\n",
    "    \n",
    ").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "pndf.printSchema\n",
    "pndf = pndf.na.fill({'Num_Prev_Cesareans': 0.0})\n",
    "#pndf = pndf.withColumn(\"Infant_Living\", blank_as_null(\"Infant_Living\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Num_Prev_Cesareans=0, Payment_Source=2, Payment_Recode=2, Five_Minute_APGAR_Score=9, Five_Minute_APGAR_Recode=4, Ten_Minute_APGAR_Score=88, Ten_Minute_APGAR_Recode=5, Plurality=1, Plurality_Imputed=None, Sex_of_Infant='F', Imputed_Sex=None, Last_Normal_Menses_Month=10, Last_Normal_Menses_Year=2012, Combined_Gestation_Imputed=None, Obstetric_Estimate_of_Gestation_Used_Flag=None, Combined_Gestation_Detail_in_Weeks=42, Combined_Gestation_Recode_Weeks=1, Birth_Weight_Recode_14=11, Birth_Weight_Recode_4=3, Imputed_Birthwieght=4119, Assisted_Ventilation='N', Assisted_Ventilation_Greater_Than_6_Hours='N', Admission_to_NICU='N', Surfactant='N', Antibiotics='N', Seizures='N', No_Abnormal_Conditions_Checked=1, Anencephaly='N', Meningomyelocele_or_Spina_Bifida='N', Cyanotic_Congenital_Heart_Disease='N', Congenital_Diaphragmatic_Hernia='N', Omphalocele='N', Gastroschisis='N', Limb_Reduction_Defect='N', Cleft_Lip_w_or_wo_Cleft_Palate='N', Cleft_Palate_alone='N', Suspected_Chromosomal_Disorder='N', Hypospadias='N', No_Congenital_Anomalies_Checked=1, Infant_Living='Y', Infant_Being_Breastfed='Y', id=0, Infant_Living_Index=0.0),\n",
       " Row(Birth_Year=2013, Birth_Month=9, Mothers_Age=27, Num_Prev_Cesareans=1, Payment_Source=1, Payment_Recode=1, Five_Minute_APGAR_Score=6, Five_Minute_APGAR_Recode=2, Ten_Minute_APGAR_Score=88, Ten_Minute_APGAR_Recode=5, Plurality=1, Plurality_Imputed=None, Sex_of_Infant='F', Imputed_Sex=None, Last_Normal_Menses_Month=12, Last_Normal_Menses_Year=2012, Combined_Gestation_Imputed=None, Obstetric_Estimate_of_Gestation_Used_Flag=None, Combined_Gestation_Detail_in_Weeks=40, Combined_Gestation_Recode_Weeks=0, Birth_Weight_Recode_14=10, Birth_Weight_Recode_4=3, Imputed_Birthwieght=3629, Assisted_Ventilation='Y', Assisted_Ventilation_Greater_Than_6_Hours='N', Admission_to_NICU='N', Surfactant='N', Antibiotics='Y', Seizures='N', No_Abnormal_Conditions_Checked=0, Anencephaly='N', Meningomyelocele_or_Spina_Bifida='N', Cyanotic_Congenital_Heart_Disease='N', Congenital_Diaphragmatic_Hernia='N', Omphalocele='N', Gastroschisis='N', Limb_Reduction_Defect='N', Cleft_Lip_w_or_wo_Cleft_Palate='N', Cleft_Palate_alone='N', Suspected_Chromosomal_Disorder='N', Hypospadias='N', No_Congenital_Anomalies_Checked=1, Infant_Living='Y', Infant_Being_Breastfed='Y', id=1, Infant_Living_Index=0.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"Infant_Living_at_Time_of_Report\", outputCol='Infant_Living_Index')\n",
    "pndf = stringIndexer.fit(pndf).transform(pndf)\n",
    "pndf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type StringType is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2012.transform.\n: java.lang.IllegalArgumentException: Data type StringType is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:121)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:117)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:117)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ea345a23d8b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpca_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Infant_Living'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpca_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvector_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpndf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvector_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type StringType is not supported.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "pca_cols = pndf.columns\n",
    "pca_cols.remove('Infant_Living_at_Time_of_Report')\n",
    "assembler = VectorAssembler(inputCols=pca_cols, outputCol='features')\n",
    "vector_df = assembler.transform(pndf)\n",
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0, features=DenseVector([2013.0, 7.0, 24.0, 10.0, 0.0, 1.0, 0.0, 0.0]), scaledFeatures=DenseVector([6292.0563, 2.0564, 3.7862, 1.0951, 0.0, 2.3585, 0.0, 0.0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol='features', \n",
    "                        outputCol='scaledFeatures', withMean=False, withStd=True) # TODO: should withMean be True??\n",
    "scaled_df = scaler.fit(vector_df).transform(vector_df)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0, features=DenseVector([2013.0, 7.0, 24.0, 10.0, 0.0, 1.0, 0.0, 0.0]), scaledFeatures=DenseVector([6292.0563, 2.0564, 3.7862, 1.0951, 0.0, 2.3585, 0.0, 0.0]), normalizeFeatures=DenseVector([0.9985, 0.0003, 0.0006, 0.0002, 0.0, 0.0004, 0.0, 0.0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "nrmlzer = Normalizer(inputCol='scaledFeatures', outputCol='normalizeFeatures', p=1.0)\n",
    "l1Normalized = nrmlzer.transform(scaled_df)\n",
    "l1Normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|features_pca                                                                          |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|[-0.9328793047880449,0.023051434768484664,-0.012615119654158995,0.003931802576143753] |\n",
      "|[-0.9327163038056777,0.023017504060830096,-0.012683599857090907,0.003917862798176857] |\n",
      "|[-0.9327612315101593,0.022997542762369504,-0.012713401868340616,0.003939066656460864] |\n",
      "|[-0.9328066253881045,0.023001673448548622,-0.012718016294802991,0.003960496036609716] |\n",
      "|[-0.9325529774210644,0.023013697442188515,-0.01273538839333199,0.0038361783630625557] |\n",
      "|[-0.9325858724385955,0.02297695878148644,-0.012744315652816437,0.003876007917191397]  |\n",
      "|[-0.9328756008287072,0.02295064860334851,-0.012709027018294985,0.004019653845095318]  |\n",
      "|[-0.9326374299555169,0.022977022983273362,-0.012798300873610504,0.00392007304286202]  |\n",
      "|[-0.9327455933665315,0.02293419776100728,-0.012744559995443928,0.0039780041744838296] |\n",
      "|[-0.9328368061704865,0.02297175431300115,-0.012789807792984277,0.004017252913695688]  |\n",
      "|[-0.9326196166009643,0.022998806989596592,-0.012825261150161736,0.003908612024188453] |\n",
      "|[-0.9321623374049559,0.023095209907641204,-0.012967977526990518,0.003965591150286569] |\n",
      "|[-0.9327170623438684,0.02330123601511437,-0.012662831480668999,0.004089046758484617]  |\n",
      "|[-0.9325870907408937,0.023284744604442385,-0.012698365551844027,0.004047393950266402] |\n",
      "|[-0.9325773375608043,0.02323704510020632,-0.012639830465604728,0.004048889599970403]  |\n",
      "|[-0.9324598782313787,0.023261431632421056,-0.012671049339326132,0.003988842516107661] |\n",
      "|[-0.9325964205624133,0.023303150069306226,-0.012720893302059288,0.004049511838839702] |\n",
      "|[-0.9321218787116893,0.0232610498516328,-0.012728553178720122,0.0038443618643891325]  |\n",
      "|[-0.9321991462247363,0.023359338787632456,-0.012868153013140435,0.004159831829411348] |\n",
      "|[-0.9326836097179985,0.023259668648226227,-0.012720953655250375,0.0041165155763425625]|\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "num_principal_comp = 4\n",
    "pca = PCA(k=num_principal_comp, inputCol='normalizeFeatures', outputCol='features_pca')\n",
    "pca_model = pca.fit(l1Normalized)\n",
    "pca_feat = pca_model.transform(l1Normalized).select('features_pca')\n",
    "pca_feat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
